{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Batch Normalization**:\n",
        "Batch Normalization (BN) is a technique used to improve the training of deep neural networks by normalizing the inputs of each mini-batch. It helps mitigate issues like vanishing/exploding gradients, stabilizes and accelerates training, and can act as a regularizer.\n"
      ],
      "metadata": {
        "id": "ID0uWwzusNlK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XDxFQ6ENpEWN",
        "outputId": "78801eca-b5b1-4779-aee9-724e416d720e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "29515/29515 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "26421880/26421880 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "5148/5148 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "4422102/4422102 [==============================] - 0s 0us/step\n"
          ]
        }
      ],
      "source": [
        "# Import necessary libraries\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "# Load the Fashion MNIST dataset\n",
        "fashion_mnist = tf.keras.datasets.fashion_mnist.load_data()\n",
        "(X_train_full, y_train_full), (X_test, y_test) = fashion_mnist\n",
        "\n",
        "# Split the dataset into training and validation sets\n",
        "X_train, y_train = X_train_full[:-5000], y_train_full[:-5000]\n",
        "X_valid, y_valid = X_train_full[-5000:], y_train_full[-5000:]\n",
        "\n",
        "# Normalize the pixel values to the range [0, 1]\n",
        "X_train, X_valid, X_test = X_train / 255.0, X_valid / 255.0, X_test / 255.0"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define class names for the Fashion MNIST dataset\n",
        "class_names = [\"T-shirt/top\", \"Trouser\", \"Pullover\", \"Dress\", \"Coat\",\n",
        "               \"Sandal\", \"Shirt\", \"Sneaker\", \"Bag\", \"Ankle boot\"]"
      ],
      "metadata": {
        "id": "1mOjuuL4qsXi"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the mean and standard deviation of the pixel values in the training set\n",
        "pixel_means = X_train.mean(axis=0, keepdims=True)\n",
        "pixel_stds = X_train.std(axis=0, keepdims=True)\n",
        "\n",
        "# Standardize the training, validation, and test sets using the mean and standard deviation of the training set\n",
        "X_train_scaled = (X_train - pixel_means) / pixel_stds\n",
        "X_valid_scaled = (X_valid - pixel_means) / pixel_stds\n",
        "X_test_scaled = (X_test - pixel_means) / pixel_stds"
      ],
      "metadata": {
        "id": "JFofMBb2qymJ"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Clear any previous TensorFlow graphs\n",
        "tf.keras.backend.clear_session()\n",
        "\n",
        "# Set the random seed for reproducibility\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "# Build a Sequential model with Batch Normalization layers\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Flatten(input_shape=[28, 28]),  # Flatten the input images (28x28 pixels) into 1D arrays\n",
        "    tf.keras.layers.BatchNormalization(),  # Batch normalization layer to normalize the input\n",
        "    tf.keras.layers.Dense(300, activation=\"relu\",\n",
        "                          kernel_initializer=\"he_normal\"),  # Fully connected layer with 300 units and ReLU activation\n",
        "    tf.keras.layers.BatchNormalization(),  # Batch normalization layer after the first dense layer\n",
        "    tf.keras.layers.Dense(100, activation=\"relu\",\n",
        "                          kernel_initializer=\"he_normal\"),  # Fully connected layer with 100 units and ReLU activation\n",
        "    tf.keras.layers.BatchNormalization(),  # Batch normalization layer after the second dense layer\n",
        "    tf.keras.layers.Dense(10, activation=\"softmax\")  # Output layer with 10 units (one for each class) and softmax activation\n",
        "])\n",
        "\n",
        "# Print the model summary to show its architecture\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nYm7B_4Oq1dV",
        "outputId": "088f3e65-aeb7-4a76-a02b-9b98358a097a"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " flatten (Flatten)           (None, 784)               0         \n",
            "                                                                 \n",
            " batch_normalization (Batch  (None, 784)               3136      \n",
            " Normalization)                                                  \n",
            "                                                                 \n",
            " dense (Dense)               (None, 300)               235500    \n",
            "                                                                 \n",
            " batch_normalization_1 (Bat  (None, 300)               1200      \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 100)               30100     \n",
            "                                                                 \n",
            " batch_normalization_2 (Bat  (None, 100)               400       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 10)                1010      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 271346 (1.04 MB)\n",
            "Trainable params: 268978 (1.03 MB)\n",
            "Non-trainable params: 2368 (9.25 KB)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile the model with sparse categorical cross-entropy loss and SGD optimizer\n",
        "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"sgd\", metrics=[\"accuracy\"])\n",
        "\n",
        "# Train the model for 2 epochs using the training set and validate using the validation set\n",
        "model.fit(X_train_scaled, y_train, epochs=2, validation_data=(X_valid_scaled, y_valid))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FYjj6WtXq4ER",
        "outputId": "992de26f-a729-431e-9e0a-db780eb64480"
      },
      "execution_count": 5,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/2\n",
            "1719/1719 [==============================] - 13s 7ms/step - loss: 0.5572 - accuracy: 0.8065 - val_loss: 0.4068 - val_accuracy: 0.8518\n",
            "Epoch 2/2\n",
            "1719/1719 [==============================] - 11s 6ms/step - loss: 0.4086 - accuracy: 0.8545 - val_loss: 0.3666 - val_accuracy: 0.8662\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7883c9eeb190>"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " Batch Normalizationccan be placed before or after the activation function. Both approaches can work, but the best choice might depend on the specific model and problem."
      ],
      "metadata": {
        "id": "F46nd3eKrsWo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Clear the name counters and set the random seed\n",
        "tf.keras.backend.clear_session()\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "# Define the model architecture using Sequential API\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Flatten(input_shape=[28, 28]),  # Flatten the input images (28x28 pixels) into 1D arrays\n",
        "    tf.keras.layers.Dense(300, kernel_initializer=\"he_normal\", use_bias=False),  # Fully connected layer with 300 units and He initialization\n",
        "    tf.keras.layers.BatchNormalization(),  # Batch normalization layer to normalize the outputs of the previous layer\n",
        "    tf.keras.layers.Activation(\"relu\"),  # ReLU activation function\n",
        "    tf.keras.layers.Dense(100, kernel_initializer=\"he_normal\", use_bias=False),  # Fully connected layer with 100 units and He initialization\n",
        "    tf.keras.layers.BatchNormalization(),  # Batch normalization layer to normalize the outputs of the previous layer\n",
        "    tf.keras.layers.Activation(\"relu\"),  # ReLU activation function\n",
        "    tf.keras.layers.Dense(10, activation=\"softmax\")  # Output layer with 10 units (one for each class) and softmax activation\n",
        "])\n",
        "\n",
        "# Compile the model with sparse categorical cross-entropy loss and SGD optimizer\n",
        "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"sgd\", metrics=[\"accuracy\"])\n",
        "\n",
        "# Train the model for 2 epochs using the training set and validate using the validation set\n",
        "model.fit(X_train_scaled, y_train, epochs=2, validation_data=(X_valid_scaled, y_valid))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kv59bIR4q8NL",
        "outputId": "843c8ae0-f926-4020-84be-8a1f82b6f58d"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/2\n",
            "1719/1719 [==============================] - 12s 6ms/step - loss: 0.6218 - accuracy: 0.7905 - val_loss: 0.4303 - val_accuracy: 0.8422\n",
            "Epoch 2/2\n",
            "1719/1719 [==============================] - 9s 5ms/step - loss: 0.4308 - accuracy: 0.8481 - val_loss: 0.3781 - val_accuracy: 0.8612\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7883c9ee89a0>"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AGNNX-73rT1O"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}